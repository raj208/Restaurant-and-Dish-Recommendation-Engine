import streamlit as st
import pandas as pd
import numpy as np
import pickle
import scipy.sparse as sparse

# ==========================================
# 1. LOAD DATA & MODELS
# ==========================================
@st.cache_resource
def load_data():
    # Load CSVs
    try:
        interactions = pd.read_csv('data/interactions.csv')
        restaurants = pd.read_csv('data/restaurants.csv')
        
        # Load Models
        with open('data/als_model.pkl', 'rb') as f:
            als_data = pickle.load(f)
            
        with open('data/content_model.pkl', 'rb') as f:
            content_data = pickle.load(f)
            
        return interactions, restaurants, als_data, content_data
    except FileNotFoundError:
        return None, None, None, None

# Load everything
df_interactions, df_restaurants, als_data, content_data = load_data()

# Handle missing data gracefully
if df_interactions is None:
    st.error("‚ùå Error: Data files not found. Please ensure 'data/' folder contains interactions.csv, restaurants.csv, and .pkl models.")
    st.stop()

# Extract model components
als_model = als_data['model']
user_mapper = als_data['user_mapper']
item_inv_mapper = als_data['item_inv_mapper']
item_mapper = als_data['item_mapper']

content_sim_matrix = content_data['similarity_matrix']
content_id_to_index = content_data['id_to_index']

# Create sparse matrix for ALS
unique_users = df_interactions['user_id'].unique()
unique_items = df_interactions['restaurant_id'].unique()
user_indices = df_interactions['user_id'].map(user_mapper).values
item_indices = df_interactions['restaurant_id'].map(item_mapper).values
confidence = df_interactions['strength'].values
sparse_user_item = sparse.csr_matrix((confidence, (user_indices, item_indices)), shape=(len(unique_users), len(unique_items)))

# ==========================================
# 2. RECOMMENDATION FUNCTIONS
# ==========================================
def get_als_recs(user_id, n=10):
    if user_id not in user_mapper:
        return []
    user_idx = user_mapper[user_id]
    ids, scores = als_model.recommend(user_idx, sparse_user_item[user_idx], N=n, filter_already_liked_items=True)
    recs = []
    for i, score in zip(ids, scores):
        if i in item_inv_mapper:
            recs.append({'restaurant_id': item_inv_mapper[i], 'als_score': float(score)})
    return recs

def get_content_recs(last_liked_item_id, n=10):
    if last_liked_item_id not in content_id_to_index:
        return []
    idx = content_id_to_index[last_liked_item_id]
    sim_scores = list(enumerate(content_sim_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    recs = []
    for i, score in sim_scores[1:n+1]:
        rest_id = df_restaurants.iloc[i]['restaurant_id']
        recs.append({'restaurant_id': rest_id, 'content_score': float(score)})
    return recs

def get_hybrid_recs(user_id):
    user_history = df_interactions[df_interactions['user_id'] == user_id]
    if user_history.empty:
        return []
    last_item = user_history.iloc[-1]['restaurant_id']
    
    als_candidates = get_als_recs(user_id, n=20)
    content_candidates = get_content_recs(last_item, n=20)
    
    scores = {}
    # Normalize ALS
    max_als = max([x['als_score'] for x in als_candidates]) if als_candidates else 1
    
    for r in als_candidates:
        rid = r['restaurant_id']
        scores[rid] = scores.get(rid, {'als': 0, 'content': 0})
        scores[rid]['als'] = r['als_score'] / max_als
        
    for r in content_candidates:
        rid = r['restaurant_id']
        scores[rid] = scores.get(rid, {'als': 0, 'content': 0})
        scores[rid]['content'] = r['content_score']
        
    final_recs = []
    for rid, s in scores.items():
        # Weighted Hybrid Score
        final_score = (0.5 * s['als']) + (0.5 * s['content'])
        final_recs.append({'restaurant_id': rid, 'score': final_score, 'debug': s})
        
    final_recs = sorted(final_recs, key=lambda x: x['score'], reverse=True)[:10]
    return final_recs

# ==========================================
# 3. STREAMLIT UI
# ==========================================
st.set_page_config(page_title="Swiggy AI Recommender", layout="wide")

# Header
st.title("üçî Swiggy-Style Recommendation Engine")
st.markdown("### A Hybrid System: Collaborative Filtering + Content-Based")

# Sidebar
user_ids = sorted(df_interactions['user_id'].unique())
selected_user = st.sidebar.selectbox("Select User ID (Simulated)", user_ids)

st.sidebar.subheader("User History (Last 3)")
history = df_interactions[df_interactions['user_id'] == selected_user].tail(3)
if not history.empty:
    for _, row in history.iterrows():
        r_name = df_restaurants[df_restaurants['restaurant_id'] == row['restaurant_id']]['name'].values[0]
        st.sidebar.text(f"‚Ä¢ {r_name} ({row['interaction_type']})")
else:
    st.sidebar.warning("No history found.")

# Tabs
tab1, tab2, tab3 = st.tabs(["üîπ Collaborative Filtering", "üî∏ Content-Based", "üöÄ Hybrid Engine"])

with tab1:
    st.subheader(f"Top Picks for User {selected_user} (Behavior Only)")
    st.caption("These items are trending among users with similar taste to you.")
    als_recs = get_als_recs(selected_user)
    if als_recs:
        for rec in als_recs:
            r_info = df_restaurants[df_restaurants['restaurant_id'] == rec['restaurant_id']].iloc[0]
            st.info(f"**{r_info['name']}** | {r_info['cuisines']} | ‚Çπ{r_info['avg_cost_for_two']}")
    else:
        st.warning("No data found.")

with tab2:
    st.subheader("Because you liked your last order...")
    user_hist = df_interactions[df_interactions['user_id'] == selected_user]
    if not user_hist.empty:
        last_item_id = user_hist.iloc[-1]['restaurant_id']
        last_item_name = df_restaurants[df_restaurants['restaurant_id'] == last_item_id]['name'].values[0]
        st.markdown(f"**Reference Item:** `{last_item_name}`")
        
        content_recs = get_content_recs(last_item_id)
        if content_recs:
            for rec in content_recs:
                r_info = df_restaurants[df_restaurants['restaurant_id'] == rec['restaurant_id']].iloc[0]
                st.success(f"**{r_info['name']}** | {r_info['cuisines']} | Match: {rec['content_score']*100:.0f}%")
    else:
        st.warning("User has no history.")

with tab3:
    st.subheader("üèÜ Hybrid Recommendations & Explainability")
    st.caption("We combine your hidden behavior patterns (ALS) with specific food preferences (Content).")
    
    hybrid_recs = get_hybrid_recs(selected_user)
    
    if hybrid_recs:
        # --- NEW: EXPLAINABILITY CHART ---
        st.markdown("#### üìä Recommendation Logic Breakdown")
        
        # Prepare Data for Chart
        chart_data = []
        for rec in hybrid_recs[:5]: # Top 5 only for cleaner chart
            r_name = df_restaurants[df_restaurants['restaurant_id'] == rec['restaurant_id']]['name'].values[0]
            chart_data.append({
                "Restaurant": r_name,
                "Behavioral Match (ALS)": rec['debug']['als'],
                "Content Match (Similarity)": rec['debug']['content']
            })
            
        chart_df = pd.DataFrame(chart_data)
        chart_df.set_index('Restaurant', inplace=True)
        
        # Display Stacked Bar Chart
        st.bar_chart(chart_df, height=300)
        st.divider()

        # --- Detailed List ---
        st.markdown("#### Detailed Recommendations")
        for rec in hybrid_recs:
            r_info = df_restaurants[df_restaurants['restaurant_id'] == rec['restaurant_id']].iloc[0]
            
            with st.container():
                c1, c2 = st.columns([3, 1])
                with c1:
                    st.markdown(f"**{r_info['name']}**")
                    st.text(f"{r_info['cuisines']} | ‚Çπ{r_info['avg_cost_for_two']}")
                with c2:
                    st.metric("Hybrid Score", f"{rec['score']:.2f}")
                st.divider()
    else:
        st.write("No hybrid recommendations available.")